{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Union, List\n",
    "from json import dumps\n",
    "from requests import get\n",
    "from http import HTTPStatus\n",
    "\n",
    "\n",
    "StructureType = Dict[str, Union[dict, str]]\n",
    "FiltersType = Iterable[str]\n",
    "APIResponseType = Union[List[StructureType], str]\n",
    "\n",
    "\n",
    "def get_paginated_dataset(filters: FiltersType, structure: StructureType,\n",
    "                          as_csv: bool = False) -> APIResponseType:\n",
    "    \"\"\"\n",
    "    Extracts paginated data by requesting all of the pages\n",
    "    and combining the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filters: Iterable[str]\n",
    "        API filters. See the API documentations for additional\n",
    "        information.\n",
    "\n",
    "    structure: Dict[str, Union[dict, str]]\n",
    "        Structure parameter. See the API documentations for\n",
    "        additional information.\n",
    "\n",
    "    as_csv: bool\n",
    "        Return the data as CSV. [default: ``False``]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[List[StructureType], str]\n",
    "        Comprehensive list of dictionaries containing all the data for\n",
    "        the given ``filters`` and ``structure``.\n",
    "    \"\"\"\n",
    "    endpoint = \"https://api.coronavirus.data.gov.uk/v1/data\"\n",
    "\n",
    "    api_params = {\n",
    "        \"filters\": str.join(\";\", filters),\n",
    "        \"structure\": dumps(structure, separators=(\",\", \":\")),\n",
    "        \"format\": \"json\" if not as_csv else \"csv\"\n",
    "    }\n",
    "\n",
    "    data = list()\n",
    "\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        # Adding page number to query params\n",
    "        api_params[\"page\"] = page_number\n",
    "\n",
    "        response = get(endpoint, params=api_params, timeout=10)\n",
    "\n",
    "        if response.status_code >= HTTPStatus.BAD_REQUEST:\n",
    "            raise RuntimeError(f'Request failed: {response.text}')\n",
    "        elif response.status_code == HTTPStatus.NO_CONTENT:\n",
    "            break\n",
    "\n",
    "        if as_csv:\n",
    "            csv_content = response.content.decode()\n",
    "\n",
    "            # Removing CSV header (column names) where page \n",
    "            # number is greater than 1.\n",
    "            if page_number > 1:\n",
    "                data_lines = csv_content.split(\"\\n\")[1:]\n",
    "                csv_content = str.join(\"\\n\", data_lines)\n",
    "\n",
    "            data.append(csv_content.strip())\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "        current_data = response.json()\n",
    "        page_data: List[StructureType] = current_data['data']\n",
    "        \n",
    "        data.extend(page_data)\n",
    "\n",
    "        # The \"next\" attribute in \"pagination\" will be `None`\n",
    "        # when we reach the end.\n",
    "        if current_data[\"pagination\"][\"next\"] is None:\n",
    "            break\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "    if not as_csv:\n",
    "        return data\n",
    "\n",
    "    # Concatenating CSV pages\n",
    "    return str.join(\"\\n\", data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    query_filters = [\n",
    "        f\"areaType=region\"\n",
    "    ]\n",
    "\n",
    "    query_structure = {\n",
    "        \"date\": \"date\",\n",
    "        \"name\": \"areaName\",\n",
    "        \"code\": \"areaCode\",\n",
    "        \"daily\": \"newCasesBySpecimenDate\",\n",
    "        \"cumulative\": \"cumCasesBySpecimenDate\"\n",
    "    }\n",
    "\n",
    "    json_data = get_paginated_dataset(query_filters, query_structure)\n",
    "    print(\"JSON:\")\n",
    "    print(f\"Length:\", len(json_data))\n",
    "    print(\"Data (first 3 items):\", json_data[:3])\n",
    "\n",
    "    print(\"---\" * 10)\n",
    "    \n",
    "    csv_data = get_paginated_dataset(query_filters, query_structure, as_csv=True)\n",
    "    csv_lines = csv_data.split(\"\\n\")\n",
    "    print(\"CSV:\")\n",
    "    print(f\"Length:\", len(csv_lines))\n",
    "    print(\"Data (first 3 lines):\", csv_lines[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class AreaTypeEnum(Enum):\n",
    "    \"\"\"\n",
    "    overview\n",
    "        Overview data for the United Kingdom\n",
    "    nation\n",
    "        Nation data (England, Northern Ireland, Scotland, and Wales)\n",
    "    region\n",
    "        Region data\n",
    "    nhsRegion\n",
    "        NHS Region data\n",
    "    utla\n",
    "        Upper-tier local authority data\n",
    "    ltla\n",
    "        Lower-tier local authority data\n",
    "    \"\"\"\n",
    "    OVERVIEW: str = \"overview\"\n",
    "    NATION: str = \"nation\"\n",
    "    REGION: str = \"region\"\n",
    "    NHS_REGION: str = \"nhsRegion\"\n",
    "    UPPER_TIER_LOCAL_AUTHORITY: str = \"utla\"\n",
    "    LOWER_TIER_LOCAL_AUTHORITY: str = \"ltla\"\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "# import geopandas as gpd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class AuthoritiesBoundaryData:\n",
    "    \"\"\"Data taken from https://data.gov.uk/dataset/d1647852-4b75-4ab2-8219-860bfef6ac9d/regions-december-2016-full-clipped-boundaries-in-england\n",
    "\n",
    "    Published by:\n",
    "        Office for National Statistics\n",
    "    Last updated:\n",
    "        12 June 2017\n",
    "    Topic:\n",
    "        Mapping\n",
    "    Licence:\n",
    "        Open Government Licence\n",
    "    Summary:\n",
    "        This file contains the digital vector boundaries for NHS Region (Geography) (NHSRG) in England as at April 2016. The boundaries available are:\n",
    "    \"\"\"\n",
    "\n",
    "    filepath = 'uk_authorities.geojson'\n",
    "    source_url = \"http://geoportal1-ons.opendata.arcgis.com/datasets/687f346f5023410ba86615655ff33ca9_1.geojson\"\n",
    "    try:\n",
    "        data = json.load(open(filepath))\n",
    "    except IOError:\n",
    "        logging.error('No region data found - downloading...')\n",
    "        response = requests.get(source_url)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(json.loads(response.text), f)\n",
    "            logging.info(f'Wrote data to {Path(filepath).absolute()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "# Streaming, so we can iterate over the response.\n",
    "def get(url, *args, **kwargs):\n",
    "    kwargs = kwargs.copy()\n",
    "    kwargs.pop('stream', None)\n",
    "    response = requests.get(url, stream=True, *args, **kwargs)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kibibyte\n",
    "    data = BytesIO()\n",
    "    with tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True) as progress_bar:\n",
    "        for chunk in response.iter_content(block_size):\n",
    "            progress_bar.update(len(chunk))\n",
    "            data.write(chunk)\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        raise IOError(\"Something went wrong\")\n",
    "    return data.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_postcodes = get(\"https://www.arcgis.com/sharing/rest/content/items/b6e6715fa1984648b5e690b6a8519e53/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nhs_postcodes.decode('utf-8', errors='ignore').split('\\n')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.arcgis.com/sharing/rest/content/items/b6e6715fa1984648b5e690b6a8519e53/data\")\n",
    "\n",
    "nhs_postcodes_csv = r.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame.from_features(AuthoritiesBoundaryData.data).geometry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class BoundaryDataWrapper:\n",
    "    try:\n",
    "        filepath = 'counties.geojson'\n",
    "        data_url = \"http://geoportal1-ons.opendata.arcgis.com/datasets/f99b145881724e15a04a8a113544dfc5_0.geojson?outSR={%22latestWkid%22:27700,%22wkid%22:27700}\"\n",
    "        data = json.load(open(filepath))\n",
    "    except IOError:\n",
    "        logging.error('No regional data found - downloading...')\n",
    "        response = requests.get(data_url)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(json.loads(response.text), f)\n",
    "            logging.info(f'Wrote data to {Path(filepath).absolute()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame.from_features(UKCountiesBoundaryDataWrapper.data).geometry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame.from_features(NHSBoundaryDataWrapper.data)\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.geometry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import COVID19Py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19Py.COVID19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_structure = {\n",
    "    \"areaType\": \"areaType\",  # Area type as string\n",
    "    \"areaName\": \"areaName\",  # Area name as string\n",
    "    \"areaCode\": \"areaCode\",  # Area Code as string\n",
    "    \"date\": \"date\",  # Date as string [YYYY-MM-DD]\n",
    "    \"hash\": \"hash\",  # Unique ID as string\n",
    "    \"newCasesByPublishDate\": \"newCasesByPublishDate\",  # New cases by publish date\n",
    "    \"cumCasesByPublishDate\": \"cumCasesByPublishDate\",  # Cumulative cases by publish date\n",
    "    \"cumCasesBySpecimenDateRate\": \"cumCasesBySpecimenDateRate\",  # Rate of cumulative cases by publish date per 100k resident population\n",
    "    \"newCasesBySpecimenDate\": \"newCasesBySpecimenDate\",  # New cases by specimen date\n",
    "    \"cumCasesBySpecimenDateRate\": \"cumCasesBySpecimenDateRate\",  # Rate of cumulative cases by specimen date per 100k resident population\n",
    "    \"cumCasesBySpecimenDate\": \"cumCasesBySpecimenDate\",  # Cumulative cases by specimen date\n",
    "    \"maleCases\": \"maleCases\",  # Male cases (by age)\n",
    "    \"femaleCases\": \"femaleCases\",  # Female cases (by age)\n",
    "    \"newPillarOneTestsByPublishDate\": \"newPillarOneTestsByPublishDate\",  # New pillar one tests by publish date\n",
    "    \"cumPillarOneTestsByPublishDate\": \"cumPillarOneTestsByPublishDate\",  # Cumulative pillar one tests by publish date\n",
    "    \"newPillarTwoTestsByPublishDate\": \"newPillarTwoTestsByPublishDate\",  # New pillar two tests by publish date\n",
    "    \"cumPillarTwoTestsByPublishDate\": \"cumPillarTwoTestsByPublishDate\",  # Cumulative pillar two tests by publish date\n",
    "    \"newPillarThreeTestsByPublishDate\": \"newPillarThreeTestsByPublishDate\",  # New pillar three tests by publish date\n",
    "    \"cumPillarThreeTestsByPublishDate\": \"cumPillarThreeTestsByPublishDate\",  # Cumulative pillar three tests by publish date\n",
    "    \"newPillarFourTestsByPublishDate\": \"newPillarFourTestsByPublishDate\",  # New pillar four tests by publish date\n",
    "    \"cumPillarFourTestsByPublishDate\": \"cumPillarFourTestsByPublishDate\",  # Cumulative pillar four tests by publish date\n",
    "    \"newAdmissions\": \"newAdmissions\",  # New admissions\n",
    "    \"cumAdmissions\": \"cumAdmissions\",  # Cumulative number of admissions\n",
    "    \"cumAdmissionsByAge\": \"cumAdmissionsByAge\",  # Cumulative admissions by age\n",
    "    \"cumTestsByPublishDate\": \"cumTestsByPublishDate\",  # Cumulative tests by publish date\n",
    "    \"newTestsByPublishDate\": \"newTestsByPublishDate\",  # New tests by publish date\n",
    "    \"covidOccupiedMVBeds\": \"covidOccupiedMVBeds\",  # COVID-19 occupied beds with mechanical ventilators\n",
    "    \"hospitalCases\": \"hospitalCases\",  # Hospital cases\n",
    "    \"plannedCapacityByPublishDate\": \"plannedCapacityByPublishDate\",  # Planned capacity by publish date\n",
    "    \"newDeaths28DaysByPublishDate\": \"newDeaths28DaysByPublishDate\",  # Deaths within 28 days of positive test\n",
    "    \"cumDeaths28DaysByPublishDate\": \"cumDeaths28DaysByPublishDate\",  # Cumulative deaths within 28 days of positive test\n",
    "    \"cumDeaths28DaysByPublishDateRate\": \"cumDeaths28DaysByPublishDateRate\",  # Rate of cumulative deaths within 28 days of positive test per 100k resident population\n",
    "    \"newDeaths28DaysByDeathDate\": \"newDeaths28DaysByDeathDate\",  # Deaths within 28 days of positive test by death date\n",
    "    \"cumDeaths28DaysByDeathDate\": \"cumDeaths28DaysByDeathDate\",  # Cumulative deaths within 28 days of positive test by death date\n",
    "    \"cumDeaths28DaysByDeathDateRate\": \"cumDeaths28DaysByDeathDateRate\",  # Rate of cumulative deaths within 28 days of positive test by death date per 100k resident population\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = get_paginated_dataset(query_filters, query_structure, as_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Union, List, Optional\n",
    "from json import dumps\n",
    "from requests import get\n",
    "from http import HTTPStatus\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "StructureType = Dict[str, Union[dict, str]]\n",
    "FiltersType = Iterable[str]\n",
    "APIResponseType = Union[List[StructureType], str]\n",
    "\n",
    "\n",
    "class AreaTypeEnum(Enum):\n",
    "    \"\"\"\n",
    "    overview\n",
    "        Overview data for the United Kingdom\n",
    "    nation\n",
    "        Nation data (England, Northern Ireland, Scotland, and Wales)\n",
    "    region\n",
    "        Region data\n",
    "    nhsRegion\n",
    "        NHS Region data\n",
    "    utla\n",
    "        Upper-tier local authority data\n",
    "    ltla\n",
    "        Lower-tier local authority data\n",
    "    \"\"\"\n",
    "    OVERVIEW: str = \"overview\"\n",
    "    NATION: str = \"nation\"\n",
    "    REGION: str = \"region\"\n",
    "    NHS_REGION: str = \"nhsRegion\"\n",
    "    UPPER_TIER_LOCAL_AUTHORITY: str = \"utla\"\n",
    "    LOWER_TIER_LOCAL_AUTHORITY: str = \"ltla\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class GovUKCoronavirusData:\n",
    "\n",
    "    DEFAULT_QUERY_STRUCTURE = {\n",
    "        \"areaType\": \"areaType\",  # Area type as string\n",
    "        \"areaName\": \"areaName\",  # Area name as string\n",
    "        \"areaCode\": \"areaCode\",  # Area Code as string\n",
    "        \"date\": \"date\",  # Date as string [YYYY-MM-DD]\n",
    "        \"hash\": \"hash\",  # Unique ID as string\n",
    "        \"newCasesByPublishDate\": \"newCasesByPublishDate\",  # New cases by publish date\n",
    "        \"cumCasesByPublishDate\": \"cumCasesByPublishDate\",  # Cumulative cases by publish date\n",
    "        \"cumCasesBySpecimenDateRate\": \"cumCasesBySpecimenDateRate\",  # Rate of cumulative cases by publish date per 100k resident population\n",
    "        \"newCasesBySpecimenDate\": \"newCasesBySpecimenDate\",  # New cases by specimen date\n",
    "        \"cumCasesBySpecimenDateRate\": \"cumCasesBySpecimenDateRate\",  # Rate of cumulative cases by specimen date per 100k resident population\n",
    "        \"cumCasesBySpecimenDate\": \"cumCasesBySpecimenDate\",  # Cumulative cases by specimen date\n",
    "        \"maleCases\": \"maleCases\",  # Male cases (by age)\n",
    "        \"femaleCases\": \"femaleCases\",  # Female cases (by age)\n",
    "        \"newPillarOneTestsByPublishDate\": \"newPillarOneTestsByPublishDate\",  # New pillar one tests by publish date\n",
    "        \"cumPillarOneTestsByPublishDate\": \"cumPillarOneTestsByPublishDate\",  # Cumulative pillar one tests by publish date\n",
    "        \"newPillarTwoTestsByPublishDate\": \"newPillarTwoTestsByPublishDate\",  # New pillar two tests by publish date\n",
    "        \"cumPillarTwoTestsByPublishDate\": \"cumPillarTwoTestsByPublishDate\",  # Cumulative pillar two tests by publish date\n",
    "        \"newPillarThreeTestsByPublishDate\": \"newPillarThreeTestsByPublishDate\",  # New pillar three tests by publish date\n",
    "        \"cumPillarThreeTestsByPublishDate\": \"cumPillarThreeTestsByPublishDate\",  # Cumulative pillar three tests by publish date\n",
    "        \"newPillarFourTestsByPublishDate\": \"newPillarFourTestsByPublishDate\",  # New pillar four tests by publish date\n",
    "        \"cumPillarFourTestsByPublishDate\": \"cumPillarFourTestsByPublishDate\",  # Cumulative pillar four tests by publish date\n",
    "        \"newAdmissions\": \"newAdmissions\",  # New admissions\n",
    "        \"cumAdmissions\": \"cumAdmissions\",  # Cumulative number of admissions\n",
    "        \"cumAdmissionsByAge\": \"cumAdmissionsByAge\",  # Cumulative admissions by age\n",
    "        \"cumTestsByPublishDate\": \"cumTestsByPublishDate\",  # Cumulative tests by publish date\n",
    "        \"newTestsByPublishDate\": \"newTestsByPublishDate\",  # New tests by publish date\n",
    "        \"covidOccupiedMVBeds\": \"covidOccupiedMVBeds\",  # COVID-19 occupied beds with mechanical ventilators\n",
    "        \"hospitalCases\": \"hospitalCases\",  # Hospital cases\n",
    "        \"plannedCapacityByPublishDate\": \"plannedCapacityByPublishDate\",  # Planned capacity by publish date\n",
    "        \"newDeaths28DaysByPublishDate\": \"newDeaths28DaysByPublishDate\",  # Deaths within 28 days of positive test\n",
    "        \"cumDeaths28DaysByPublishDate\": \"cumDeaths28DaysByPublishDate\",  # Cumulative deaths within 28 days of positive test\n",
    "        \"cumDeaths28DaysByPublishDateRate\": \"cumDeaths28DaysByPublishDateRate\",  # Rate of cumulative deaths within 28 days of positive test per 100k resident population\n",
    "        \"newDeaths28DaysByDeathDate\": \"newDeaths28DaysByDeathDate\",  # Deaths within 28 days of positive test by death date\n",
    "        \"cumDeaths28DaysByDeathDate\": \"cumDeaths28DaysByDeathDate\",  # Cumulative deaths within 28 days of positive test by death date\n",
    "        \"cumDeaths28DaysByDeathDateRate\": \"cumDeaths28DaysByDeathDateRate\",  # Rate of cumulative deaths within 28 days of positive test by death date per 100k resident population\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_latest_data(cls, filters: Optional[FiltersType] = None,\n",
    "                        structure: Optional[StructureType] = None) -> dict:\n",
    "        if filters is None:\n",
    "            filters = [f'areaType={AreaTypeEnum.REGION.value}']\n",
    "        if structure is None:\n",
    "            structure = cls.DEFAULT_QUERY_STRUCTURE\n",
    "        return cls.get_paginated_dataset(filters, structure)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_paginated_dataset(filters: FiltersType, structure: StructureType,\n",
    "                              as_csv: bool = False) -> APIResponseType:\n",
    "        \"\"\"\n",
    "        Extracts paginated data by requesting all of the pages\n",
    "        and combining the results.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filters: Iterable[str]\n",
    "            API filters. See the API documentations for additional\n",
    "            information.\n",
    "\n",
    "        structure: Dict[str, Union[dict, str]]\n",
    "            Structure parameter. See the API documentations for\n",
    "            additional information.\n",
    "\n",
    "        as_csv: bool\n",
    "            Return the data as CSV. [default: ``False``]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Union[List[StructureType], str]\n",
    "            Comprehensive list of dictionaries containing all the data for\n",
    "            the given ``filters`` and ``structure``.\n",
    "        \"\"\"\n",
    "        endpoint = \"https://api.coronavirus.data.gov.uk/v1/data\"\n",
    "\n",
    "        api_params = {\n",
    "            \"filters\": str.join(\";\", filters),\n",
    "            \"structure\": dumps(structure, separators=(\",\", \":\")),\n",
    "            \"format\": \"json\" if not as_csv else \"csv\"\n",
    "        }\n",
    "\n",
    "        data = list()\n",
    "\n",
    "        page_number = 1\n",
    "\n",
    "        while True:\n",
    "            # Adding page number to query params\n",
    "            api_params[\"page\"] = page_number\n",
    "\n",
    "            response = get(endpoint, params=api_params, timeout=10)\n",
    "\n",
    "            if response.status_code >= HTTPStatus.BAD_REQUEST:\n",
    "                raise RuntimeError(f'Request failed: {response.text}')\n",
    "            elif response.status_code == HTTPStatus.NO_CONTENT:\n",
    "                break\n",
    "\n",
    "            if as_csv:\n",
    "                csv_content = response.content.decode()\n",
    "\n",
    "                # Removing CSV header (column names) where page\n",
    "                # number is greater than 1.\n",
    "                if page_number > 1:\n",
    "                    data_lines = csv_content.split(\"\\n\")[1:]\n",
    "                    csv_content = str.join(\"\\n\", data_lines)\n",
    "\n",
    "                data.append(csv_content.strip())\n",
    "                page_number += 1\n",
    "                continue\n",
    "\n",
    "            current_data = response.json()\n",
    "            page_data: List[StructureType] = current_data['data']\n",
    "\n",
    "            data.extend(page_data)\n",
    "\n",
    "            # The \"next\" attribute in \"pagination\" will be `None`\n",
    "            # when we reach the end.\n",
    "            if current_data[\"pagination\"][\"next\"] is None:\n",
    "                break\n",
    "\n",
    "            page_number += 1\n",
    "\n",
    "        if not as_csv:\n",
    "            return data\n",
    "\n",
    "        # Concatenating CSV pages\n",
    "        return str.join(\"\\n\", data)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    query_filters = [\n",
    "        f\"areaType=region\"\n",
    "    ]\n",
    "\n",
    "    query_structure = {\n",
    "        \"date\": \"date\",\n",
    "        \"name\": \"areaName\",\n",
    "        \"code\": \"areaCode\",\n",
    "        \"daily\": \"newCasesBySpecimenDate\",\n",
    "        \"cumulative\": \"cumCasesBySpecimenDate\"\n",
    "    }\n",
    "\n",
    "    json_data = get_paginated_dataset(query_filters, query_structure)\n",
    "    print(\"JSON:\")\n",
    "    print(f\"Length:\", len(json_data))\n",
    "    print(\"Data (first 3 items):\", json_data[:3])\n",
    "\n",
    "    print(\"---\" * 10)\n",
    "    \n",
    "    csv_data = get_paginated_dataset(query_filters, query_structure, as_csv=True)\n",
    "    csv_lines = csv_data.split(\"\\n\")\n",
    "    print(\"CSV:\")\n",
    "    print(f\"Length:\", len(csv_lines))\n",
    "    print(\"Data (first 3 lines):\", csv_lines[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GovUKCoronavirusData.get_latest_data(filters=['date=2020-09-01'])\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uk_covid19 import Cov19API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "df = pd.DataFrame()\n",
    "for area in tqdm(AreaTypeEnum):\n",
    "    api = Cov19API(filters=[f\"areaType={area.value}\"], structure=GovUKCoronavirusData.DEFAULT_QUERY_STRUCTURE)\n",
    "    df = pd.concat([df, api.get_dataframe()], sort=False)\n",
    "\n",
    "df.areaName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.covidOccupiedMVBeds.fillna(0) > 0].covidOccupiedMVBeds.iloc[:200].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Cov19API(filters=[\"areaType=nation\"], structure=GovUKCoronavirusData.DEFAULT_QUERY_STRUCTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_df = api.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://api.coronavirus.data.gov.uk/v1/data?filters=areaName=United%2520Kingdom;areaType=overview&structure=%7B%22areaType%22:%22areaType%22,%22areaName%22:%22areaName%22,%22areaCode%22:%22areaCode%22,%22date%22:%22date%22,%22newPillarOneTestsByPublishDate%22:%22newPillarOneTestsByPublishDate%22,%22newPillarTwoTestsByPublishDate%22:%22newPillarTwoTestsByPublishDate%22,%22newPillarThreeTestsByPublishDate%22:%22newPillarThreeTestsByPublishDate%22,%22newPillarFourTestsByPublishDate%22:%22newPillarFourTestsByPublishDate%22,%22newTestsByPublishDate%22:%22newTestsByPublishDate%22,%22cumPillarOneTestsByPublishDate%22:%22cumPillarOneTestsByPublishDate%22,%22cumPillarTwoTestsByPublishDate%22:%22cumPillarTwoTestsByPublishDate%22,%22cumPillarThreeTestsByPublishDate%22:%22cumPillarThreeTestsByPublishDate%22,%22cumPillarFourTestsByPublishDate%22:%22cumPillarFourTestsByPublishDate%22,%22cumTestsByPublishDate%22:%22cumTestsByPublishDate%22%7D&format=json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.areaType == 'utla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
